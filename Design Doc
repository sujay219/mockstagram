1. Worker Node, Producer
Kafka is used here
    - as it handles high throughput (up to 16.7k rps), durable, horizontally scalable.

Polls the Load Balancer/Task Coordinator to fetch a list of influencers to handle.

Runs a scheduler (e.g., APScheduler) to fetch data for assigned influencers every minute.

Posts new follower counts to a message queue.

-- used setInterval() clearInterval() for cron job, but instead we can use cron library as well.

2. Message Queue & Stream processor, Consumer

Handles incoming follower count updates as events.
Decouples data ingestion from data storage.

Enables retries and fault tolerance, by writing the faulty writes into a file and periodically uploading that as well. (Failover case is not tested now)

3. Database layer: InfluxDb
Light-weight and faster for time-based queries.
Other options:
    TimescaleDb is another option which can be used, but it is slower than this.
    Prometheus also is a great option but it is not for very long storage and it's pull based system.
    In this we have used Kafka consumer to pass the stream of data

4. Load Balancer / Task Coordinator
Distributes influencer ranges among nodes using:

Hash partitioning of influencer PKs.

ZooKeeper / Redis / etcd for coordination.

5. API Layer
Exposes:

Current follower count
Historical time-series (for charting)
Average follower count

This is implemented inside the InfluxDb micro service only for now. Can be made separately.

Things to be implemented:
1. Containerization of producer/poller, consumer/populator/batch handler, 
2. Using pm2 instead of node app.js
3. Batch processing of failed case and optimization of that.
4. Writing load testing framework.

Query monitoring and calculations:
1. influx db

Push every 1000 followers in one go.

USE _internal;
SELECT * FROM "runtime" WHERE time > now() - 5m;
SELECT * FROM "mem" WHERE time > now() - 5m;
SELECT * FROM "queryExecutor" WHERE time > now() - 5m;

2. Monitor the kafka failures and CPU usage of k8s pod.

Set limit to kafka poller

resources:
  requests:
    cpu: "100m"
  limits:
    cpu: "500m"

Create a Horizontal pod autoscaler:
- If average CPU usage across pods exceeds 50%, the HPA will scale ou
kubectl autoscale deployment your-deployment-name \
  --cpu-percent=70 \
  --min=2 \
  --max=50

